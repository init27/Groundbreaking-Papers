{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIYdn1woOS1n",
    "outputId": "d6a3872e-734e-4674-b8a0-0290d85d98f5"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics,\n",
    "    datasets,\n",
    "    mixed_precision,\n",
    "    optimizers,\n",
    "    callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    \"dataset_name\": \"CIFAR-10\",\n",
    "    \"image_size\": 32,\n",
    "    \"target_size\": 72,\n",
    "    \"patch_size\": 9,\n",
    "    \"num_mixer_layers\": 4,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"channels_mlp_dim\": 128,\n",
    "    \"num_classes\": 10,\n",
    "    \"dropout\": 0.25,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 50,\n",
    "    \"label_smoothing\": 1e-3,\n",
    "    \"mixed_precision\": True,\n",
    "    \"class_names\": [\n",
    "        \"airplane\", \"automobile\", \"bird\", \"cat\",\n",
    "        \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGS[\"mixed_precision\"]:\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lbRgIu0BbTF"
   },
   "outputs": [],
   "source": [
    "def get_cifar10(num_classes: int):\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_cifar10(num_classes=10)\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-Mixer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6qEXvcVR7ZJ"
   },
   "outputs": [],
   "source": [
    "def get_preprocessing_layer(\n",
    "    data_batch: Union[np.ndarray, tf.Tensor], target_size: int\n",
    ") -> Sequential:\n",
    "    normalization = preprocessing.Normalization()\n",
    "    normalization.adapt(data_batch)\n",
    "    resize = preprocessing.Resizing(target_size, target_size)\n",
    "    return Sequential([normalization, resize], name=\"preprocessing\")\n",
    "\n",
    "\n",
    "def get_augmentation_layer() -> Sequential:\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            preprocessing.RandomFlip(\"horizontal\"),\n",
    "            preprocessing.RandomRotation(factor=0.02),\n",
    "            preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"augmentation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8cOi71NDCPC"
   },
   "outputs": [],
   "source": [
    "def patch_embedding(\n",
    "    inputs: tf.Tensor, embedding_dim: int, patch_size: int\n",
    ") -> tf.Tensor:\n",
    "    x = layers.Conv2D(\n",
    "        embedding_dim,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        name=\"patch_embedding_conv2d\"\n",
    "    )(inputs)\n",
    "    return layers.Reshape(\n",
    "        (x.shape[1] * x.shape[2], x.shape[3]),\n",
    "        name=\"patch_embedding_reshape\"\n",
    "    )(x)\n",
    "\n",
    "\n",
    "def mlp_block(inputs: tf.Tensor, mlp_dim: int, name: str) -> tf.Tensor:\n",
    "    x = layers.Dense(mlp_dim, name=name + \"_dense_1\")(inputs)\n",
    "    x = layers.Activation(\"gelu\", name=name + \"_gelu_activation\")(x)\n",
    "    return layers.Dense(x.shape[-1], name=name + \"_dense_2\")(x)\n",
    "\n",
    "\n",
    "def mixer_block(\n",
    "    inputs: tf.Tensor, tokens_mlp_dim: int, channels_mlp_dim: int, name: str\n",
    ") -> tf.Tensor:\n",
    "    y = layers.LayerNormalization(name=name + \"_layer_norm_1\")(inputs)\n",
    "    y = layers.Permute((2, 1), name=name + \"_swap_axes_1\")(y)\n",
    "    # Token Mixing\n",
    "    y = mlp_block(y, tokens_mlp_dim, name=name + \"_mlp_block_1\")\n",
    "    y = layers.Permute((2, 1), name=name + \"_swap_axes_2\")(y)\n",
    "    x = layers.Add(name=name + \"_skip_connection_token\")([inputs, y])\n",
    "    # Channel Mixing\n",
    "    y = layers.LayerNormalization(name=name + \"_layer_norm_2\")(x)\n",
    "    y = mlp_block(y, channels_mlp_dim, name=name + \"_mlp_block_2\")\n",
    "    return layers.Add(name=name + \"_skip_connection_channel\")([x, y])\n",
    "\n",
    "\n",
    "def get_mlp_mixer_model(\n",
    "    num_mixer_blocks: int,\n",
    "    patch_size: int,\n",
    "    embedding_dim: int,\n",
    "    channels_mlp_dim: int,\n",
    "    num_classes: int,\n",
    "    preprocessing_layer: Union[Sequential, None],\n",
    "    augmentation_layer: Union[Sequential, None],\n",
    ") -> Model:\n",
    "    inputs = Input(\n",
    "        shape=(CONFIGS[\"image_size\"], CONFIGS[\"image_size\"], 3), name=\"Input\"\n",
    "    )\n",
    "    preprocessed_inputs = (\n",
    "        preprocessing_layer(inputs) if preprocessing_layer is not None else inputs\n",
    "    )\n",
    "    augmented_inputs = (\n",
    "        augmentation_layer(preprocessed_inputs)\n",
    "        if augmentation_layer is not None\n",
    "        else preprocessed_inputs\n",
    "    )\n",
    "    x = patch_embedding(augmented_inputs, embedding_dim, patch_size)\n",
    "    tokens_mlp_dim = x.shape[-2]\n",
    "    for idx in range(num_mixer_blocks):\n",
    "        x = mixer_block(\n",
    "            x, tokens_mlp_dim, channels_mlp_dim, name=f\"mixer_block_{idx}\"\n",
    "        )\n",
    "    x = layers.LayerNormalization(name=\"layer_norm_post_mixer\")(x)\n",
    "    x = layers.Dropout(CONFIGS[\"dropout\"], name=\"dropout\")(x)\n",
    "    x = layers.GlobalAveragePooling1D(name=\"global_average_pooling\")(x)\n",
    "    outputs = layers.Dense(\n",
    "        num_classes, activation=\"softmax\", dtype=\"float32\", name=\"output\"\n",
    "    )(x)\n",
    "    return Model(inputs, outputs, name=\"mlp_mixer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(project='mlp-mixer', name=\"cifar-10-mixed-precision\", config=CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mlp_mixer_model(\n",
    "    num_mixer_blocks=CONFIGS[\"num_mixer_layers\"],\n",
    "    patch_size=CONFIGS[\"patch_size\"],\n",
    "    embedding_dim=CONFIGS[\"embedding_dim\"],\n",
    "    channels_mlp_dim=CONFIGS[\"channels_mlp_dim\"],\n",
    "    num_classes=CONFIGS[\"num_classes\"],\n",
    "    preprocessing_layer=get_preprocessing_layer(\n",
    "        data_batch=x_train, target_size=CONFIGS[\"target_size\"]\n",
    "    ),\n",
    "    augmentation_layer=get_augmentation_layer(),\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=CONFIGS[\"learning_rate\"]),\n",
    "    loss=losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        metrics.TopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
    "        metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback = WandbCallback(\n",
    "    data_type='image',\n",
    "    save_model=True,\n",
    "    training_data=(x_train, y_train),\n",
    "    validation_data=(x_test, y_test),\n",
    "    labels=CONFIGS[\"class_names\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=CONFIGS[\"batch_size\"],\n",
    "    epochs=CONFIGS[\"epochs\"],\n",
    "    validation_split=0.1,\n",
    "    callbacks=[wandb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, top_3_accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "wandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = model.get_layer(\"patch_embedding_conv2d\").get_weights()[0]\n",
    "projections_min, projections_max = projections.min(), projections.max()\n",
    "projections = (projections - projections_min) / (projections_max - projections_min)\n",
    "table_data = []\n",
    "for idx in range(CONFIGS[\"embedding_dim\"]):\n",
    "    table_data.append([idx, wandb.Image(projections[:, :, :, idx])])\n",
    "wandb.log(\n",
    "    {\n",
    "        \"Projection of Patches\": wandb.Table(\n",
    "            data=table_data,\n",
    "            columns=[\"Index\", \"Projection\"]\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', \n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'early_terminate':{\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 5,\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [256, 512, 1024]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'label_smoothing': {\n",
    "            'values': [0.0, 0.01, 0.001]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.25, 0.5]\n",
    "        },\n",
    "        'num_mixer_layers': {\n",
    "            'values': [2, 3, 4, 5, 6]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train_fn():\n",
    "    wandb.init(\n",
    "        project='mlp-mixer',\n",
    "        config=CONFIGS\n",
    "    )\n",
    "    wandb.config.epochs = 15\n",
    "    model = get_mlp_mixer_model(\n",
    "        num_mixer_blocks=wandb.config.num_mixer_layers,\n",
    "        patch_size=wandb.config.patch_size,\n",
    "        embedding_dim=wandb.config.embedding_dim,\n",
    "        channels_mlp_dim=wandb.config.channels_mlp_dim,\n",
    "        num_classes=wandb.config.num_classes,\n",
    "        preprocessing_layer=get_preprocessing_layer(\n",
    "            data_batch=x_train, target_size=wandb.config.target_size\n",
    "        ),\n",
    "        augmentation_layer=get_augmentation_layer(),\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=wandb.config.learning_rate),\n",
    "        loss=losses.CategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            metrics.TopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
    "            metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    wandb_callback = WandbCallback(\n",
    "        data_type='image',\n",
    "        save_model=True,\n",
    "        training_data=(x_train, y_train),\n",
    "        validation_data=(x_test, y_test),\n",
    "        labels=wandb.config.class_names\n",
    "    )\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[wandb_callback]\n",
    "    )\n",
    "    loss, accuracy, top_3_accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    wandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"mlp-mixer\")\n",
    "wandb.agent(sweep_id, function=sweep_train_fn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "MLP_Mixer_Training",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
